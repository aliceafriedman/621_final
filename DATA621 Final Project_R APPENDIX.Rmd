---
title: "DATA621 Final Project: Predicting Academic Achievement"
author: Group 5
output: pdf_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE, message = FALSE)
library(gridExtra)
library(psych)
library(corrplot)
library(RColorBrewer)
library(tidyr)
library(ggplot2)
library(MASS)
library(rpart)
library(knitr)
library(ggplot2)
library(dplyr)
library(randomForest)
library(Metrics)
library(caret)
```
## Abstract 

This study investigates the external factors affecting students' academic performance in secondary schools' math and Portuguese language courses. Using a quantitative data set obtained through a survey, the study analyzes various external variables, such as students' health, social and demographic factors, study habits, and other relevant factors to understand their impact on academic performance.

The study uses Multiple Linear Regression, Random Forrest, and a Decision Tree to examine the relationship between a dependent variable (final grade) and multiple independent variables. Ultimately the non-parametric results have more predictive value and success in modelling this complex phenomenon. The results of this study indicate that future researchers should consider nonparametric approaches to gain greater predictive value.


## Introduction 

This study may be relevant to you as it explores the external factors that have an impact on students' grades. By analyzing the various external variables that can influence academic performance, this study provides valuable insights that can help parents, educators, and policymakers to better support students' success in school. 

The data used in this study was collected from a survey of secondary school students and includes information on various external variables and their impact on students' grades. The data covers two schools and includes grades for two core subjects, math and language, providing a cross-sectional understanding of the factors that influence student performance.


## Literature Review

Academic success is a widely studied phenomenon. Previous studies have used Pearsonâ€™s partial correlations, moderated linear regression, or analysis of variance between groups (ANOVA) to determine the relationship between factors such as demographics (e.g. sex), lifestyle habits (e.g. screen time), and physical and cognitive abilities (e.g. cardiovascular fitness measured by VO2 Max) to predict academic achievement. All of the studies presented below attempt to understand factors associated with a *student* rather than a school or teacher that my predict academic success, and so (as is the case with our data set) look at cross-section of students at a single school, so that the school itself is not considered a factor. This is different from our data set which includes two high schools. 

A large number of factors  have been identified in previous studies to predict academic achievement including sex, screen time, sleep, cell-phone use, maternal education, IQ; however, the effect typically found is low (explaining between approximately 6% and 50% of variation). This indicates that other factors not included in the study (which may include student-specific habits like study habits or school-related factors like school quality) may impact variation, and that the true model is likely complex.

Most studies in the literature reviewed take a linear approach; however, as academic achievement is a complex phenomenon, new nonparametric approaches not previously available to researchers may prove more fruitful.

Additional details of the studies reviewed appear in the Appendix.

## Data Source

The data source is from a Kaggle competition and represents observations from 662 students at two high schools in Portugal. The data is initially split into two data sets based on which subject is being observed (Portuguese language or math). 

We will use the data set to predict academic achievement represented by the final grades, `final_grades`. A data dictionary is included in Appendix 1.


## Data Exploration
### Data Preparation
The first task is to merge the data sets, identifying which students are identical and which grades correspond to which class. In order to predict achievement in school independent of subject, with `df_both`, using a target `final_grade`, which is the average of both classes. 

### Adding new columns
Fields which are specific to one class or another are aggregated or merged in the final data set:
`final_grade`: an average of the final grade for both classes
`absences_mean`:  average absences reported across both classes
`paid_tutor`: this column is marked as "yes" if the student receives paid tutoring in either subject
`missed_exam`: this column indicates whether a student missed an exam in any class  

We additionally added the following fields to deal with multicollinearity found in the initial data analysis:
`parentEdu`: sum of `Medu` + `Fedu` to remove a co-linearity and combine parental education into a single variable
`studentAlc`: combination of weekly and daily student alcohol consumption 


### Remove rows
If the student received a 0 on the final exam, we remove them as they received their final grade due to a missing exam as opposed to other factors. This is used to filter out students from the final analysis as this grade is presumed to indicate absence rather than student potential for achievement 

### Missing values
The data set is complete with no missing values; however in merging the two data sets we had to make a decision for how to deal with variables that are subject specific. this treatment is described above. 

### Renaming columns
We change `famsup` to `famEdsup`, and change `goout` to `friendtime` so that it is easier to understand. We also change the name of binary values to reflect the 1 value, e.g `sex` becomes `sex_F`.

### Replacing binary values
For easier computation and interpretation of results, we replace all binary data ("yes", "no"; "rural" or "urban"; "m" or "f") with 1 or 0.

```{r merge, results='hide'}
df_mat <- read.csv("https://raw.githubusercontent.com/seung-m1nsong/602/main/Final/student-mat.csv")

df_por <- read.csv("https://raw.githubusercontent.com/seung-m1nsong/602/main/Final/student-por.csv")

df_both <- dplyr::inner_join(
  df_mat %>% 
    rename(
      G1_mat = G1,
      G2_mat = G2,
      G3_mat = G3, 
      absence_mat = absences,
      failures_mat = failures,
      paid_mat = paid
    ), 
  df_por%>% 
    rename(
      G1_por = G1,
      G2_por = G2,
      G3_por = G3, 
      absence_por = absences,
      failures_por = failures,
      paid_por = paid
    )
  ) %>% rowwise() %>%
      mutate(
        absences_mean = mean(absence_mat, absence_por),
        final_grade = mean(G3_mat, G3_por),
        failures = mean(failures_mat, failures_por),
        paid_tutor = if_else(paid_mat=="yes" | paid_por=="yes", "yes", "no"),
        missed_exam = as.numeric(if_else(G3_por==0 | G3_mat==0, 1, 0)) 
      )
  
```

```{r new-vars}
df_both <- df_both %>% 
  mutate(
  parentEdu = (Medu + Fedu),
  alc = (Dalc + Walc)) %>%
    select(-Dalc, -Walc, -Medu, -Fedu)
```

```{r binary}

binary <- function(df, col, level1="yes", level0="no"){
  df[[col]][df[[col]]==level1] <- 1
  df[[col]][df[[col]]==level0] <- 0

  return(df[[col]] %>% as.numeric())
}

df_both$schoolsup <- binary(df_both, "schoolsup")

df_both$famsup <- binary(df_both, "famsup")

df_both$paid_tutor <- binary(df_both, "paid_tutor")

df_both$activities <- binary(df_both, "activities")

df_both$nursery <- binary(df_both, "nursery")

df_both$higher <- binary(df_both, "higher")

df_both$internet <- binary(df_both, "internet")

df_both$romantic <- binary(df_both, "romantic")

df_both$sex <- binary(df_both, "sex", "F", "M")

df_both$Pstatus <- binary(df_both, "Pstatus", "A", "T")

df_both$address <- binary(df_both, "address", "R", "U")

df_both$famsize <- binary(df_both, "famsize", "GT3", "LE3")

df_both$school <- binary(df_both, "school", "GP", "MS")
```

```{r rename}
df_both <- df_both %>% rename(
  famEdsup = famsup,
  sex_F = sex,
  friedtime = goout,
  rural = address,
  par_apart = Pstatus,
  friendtime = goout,
  fam_large = famsize,
  school_GP = school
)
  
```

```{r remove}
df_both <- df_both %>% filter(missed_exam == 0) %>% select(-missed_exam)
```

```{r descriptions}
descrip <- describe(df_both)
```
### Prepared Data Set EDA
The resulting data set includes:
  *`r nrow(df_both)` rows  with `r length(df_both)` columns.
  * Various features of the student data set such as personal and family characteristics, academic performance, and social activities: **school**, **sex**, **age**, **address**, **family size**, **parents' education**, **mother and father's occupation**, **travel time**, **study time**, **number of failures**, **support received from school**, **family**, **extra-curricular**, **activities**, **health**, and **academic grades**.

### Overall Statistics

When examining the distinctive variables in the **df_both**, a few noteworthy ones stand out:

* failures:
  * The average is shown as `r round(mean(df_both$failures), 2)`, but considering that the maximum value is `r max(df_both$failures)`, it seems that some students have failed in several subjects in the previous semester. This excludes students who missed their fianl exam, as descibed above.
  * The skew value is `r round(descrip["failures", "skew"], 2)`, which is a very large positive value, indicating that the data is skewed to the right.

* travel time:
  * The average is `r round(descrip["traveltime", "mean"], 2)`, and it seems that most students take a relatively short time to school.
  * The skewness value is `r round(descrip["traveltime", "skew"], 2)`, which is positive and indicates that the data is skewed to the right.

* absences:
  * The average is `r round(descrip["absences_mean", "mean"],2)`, indicating that on average students missed about 4 days.
  * Since the maximum value is `r round(descrip["absences", "max"],2)`, which is a very large value, it is possible that some students have many absences.
  * The skewness value is `r round(descrip["absences_mean", "skew"], 2)`, which is a very large positive value, indicating that the data is very skewed to the right.

* alc (drinking):
  * The average is `r round(descrip["alc", "mean"], 2)`, and most students seem to be drinking relatively little on workdays.
  * The skewness value is `r round(descrip["alc", "skew"], 2)`, which is slightly positive and indicates that the data is skewed to the right.


* health:
  * The average is `r round(descrip["health", "mean"], 2)`, and the average health status of students seems to be in the middle.
  * A negative skewness value of `r round(descrip["health", "skew"], 2)` indicates that the data is slightly skewed to the left.

`A full descriptive summary of variables in included in Appendix 2. 

### Basic Plots
Basic plots of numeric variables help us understand if they have a normal distribution -- necessary to undestand if a particular model type will be appropriate. In general, we can see most features are not normally distributed, meaning that linear regression is not likely to have good predictive value. Basic plots are included in Appendix 3.


### Correlation

When considering the distribution of *traveltime*, *studytime*, *failures*,	*famrel*, *freetime*, *goout*, *Dalc*, *Walc*, *health*, and *absences* across different categories, it becomes evident that these variables show a relatively even distribution within each G3 group. Grades and absences between the classse are highly correlated, so it makes sense to combine these for futher analysis.

We establish the presences or absence of multicollinearity in order to again determine the suitability of different model approaches, or determine if we should consider dropping some variables.

We can see that mother's and father's education (`Medu` and `Fadu`) as well as daily and weekly alcholol consumption (`Dalc` and `Walc`) are highly correlate.

```{r corrplot}
df_both %>% 
  select(-final_grade, -ends_with("_por"), -ends_with("_mat")) %>% select_if(is.numeric) %>%
  cor() %>% 
  corrplot(
  type="upper", diag = FALSE
  )
```

## Data Visualization
We can use box plots to understand the impact of factor variables on the target final grade. We can see that `failures` jumps out as having significant predictive value, whle the presence of absence of a paid tutor makes little difference

```{r boxplots}
p1 <- df_both %>% ggplot(aes(factor(traveltime), final_grade)) + geom_boxplot() + xlab("Travel Time")
p2 <- df_both %>% ggplot(aes(factor(studytime), final_grade)) + geom_boxplot() + xlab("Study Time")
p3 <- df_both %>% ggplot(aes( factor(failures), final_grade)) + geom_boxplot() + xlab("Failures")
p4 <- df_both %>% ggplot( aes(factor(famrel), final_grade)) + geom_boxplot() + xlab("Fam Rel")
p5 <- df_both %>% ggplot( aes(factor(freetime), final_grade)) + geom_boxplot()+ xlab("Freetime")
p6 <- df_both %>% ggplot( aes(factor(parentEdu), final_grade)) + geom_boxplot() + xlab("Parent Edu")
p7 <- df_both %>% ggplot( aes(factor(alc), final_grade)) + geom_boxplot() + xlab("Alcohol")
p8 <- df_both %>% ggplot( aes(factor(school_GP), final_grade)) + geom_boxplot() + xlab("School is GP")
p9 <- df_both %>% ggplot( aes(factor(health), final_grade)) + geom_boxplot() + xlab("Health")
p10<- df_both %>% ggplot( aes(factor(paid_tutor), final_grade)) + geom_boxplot() + xlab("Paid Tutuor")
p11<- df_both %>% ggplot( aes(factor(rural), final_grade)) + geom_boxplot() + xlab("Rural")
p12<- df_both %>% ggplot( aes(factor(sex_F), final_grade)) + geom_boxplot() + xlab("Female")
p13<- df_both %>% ggplot( aes(factor(fam_large), final_grade)) + geom_boxplot() + xlab("Big Fam")
p14<- df_both %>% ggplot( aes(factor(romantic), final_grade)) + geom_boxplot() + xlab("Romantic")
p15<- df_both %>% ggplot( aes(factor(internet), final_grade)) + geom_boxplot() + xlab("Internet")
p16<- df_both %>% ggplot( aes(factor(par_apart), final_grade)) + geom_boxplot() + xlab("Parents Sep.")
p17<- df_both %>% ggplot( aes(factor(friendtime), final_grade)) + geom_boxplot() + xlab("Friend Time")
p18<- df_both %>% ggplot( aes(factor(activities), final_grade)) + geom_boxplot() + xlab("Activities")

grid.arrange(p1, p2, p3, p4, p5, p6, p7,p8,p9,p10, p11, p12,p13, p14, p15, p16, p17, p18, ncol=6) 
```

In order to determine how best to measure overall student achievement across the classes, we evaluate the correlation between `G3_mat` and `G3_por`. In general, it appears that these correlation between these two subjects is quite high, especially if we omit final grades of "0" from consideration; this is helpful because it gives us a sense of the upper bound of how much variation in achievement can be explained by other features -- this correlation, after all, is for final grades for different classes but for the same students.

```{r cor_grades}
x <- "Portuguese Final Grade"
y <- "Math Final Grade"
final_grades_coef <- cor(df_both$G3_mat, df_both$G3_por, use="pairwise.complete.obs")
pg1 <- df_both %>% na.omit() %>% ggplot(aes(G3_mat, G3_por)) + 
  geom_point(position="jitter") +
  ggtitle(
    paste(
    "Correlation between final grades in Math and Portuguese,",
    round(final_grades_coef, 2)
    )) + 
  xlab(x) + 
  ylab(y)
  

nonzeroes <- df_both %>% na.omit() %>% subset(G3_por > 0 & G3_mat >0)
final_grades_coef <- cor(nonzeroes$G3_mat, nonzeroes$G3_por, use="pairwise.complete.obs")
pg2 <- nonzeroes  %>%
  ggplot(aes(G3_mat, G3_por)) + 
  geom_point(position="jitter") +
  ggtitle(
    paste(
    "Correlation between final grades omitting grades of zero,",
    round(final_grades_coef, 2)
    )) + 
  xlab(x) + 
  ylab(y)
 grid.arrange(pg1, pg2)
```

Additional data visualizaton can be found in the Appendix.

## Model Builging and Evaluation
We will attempt to predict the target, `final_grade` using three approaches: multiple linear regression, random forest, and a decision tree, and evaluate results based on validation with a test set withheld from model building.

```{r partition, results='hide'}
set.seed(123)
# Create a list of indices for the 3 folds that can be used for all models
folds <- createFolds(df_both$final_grade, k = 3, list = TRUE, returnTrain = TRUE)
train_data <- df_both[folds[[1]], ] %>% select(-ends_with("_por"), -ends_with("_mat")) #drop unwanted features
test_data <- df_both[-folds[[1]], ]
```

### Model 1 


**Multiple Linear Regrssion**
We will first attempt the simplest case multiple linear regression using the full subset method to choose the best linear model. Using the `leaps` package and 3-fold cross validation from the `caret` package, we can select the best performing model among possible linear regression strategies.

What is very interesting about this result is that the same best subset is not created for each fold -- this indicates that the relationships are not especially strong, except for those few that appear each time. The only consistent result with this methodology is to include `failures`: Students who have previously failed a class are more likely to fail again. 

A visualization and ANOVA confirm this results -- students who have previously failed are significantly likely to have a lower final grade than students who have never failed.

By evaluating the BIC curves, we can determine that the minimum BIC is between 3 and 6 features. Using the rule of thumb of the fewest features within 1 SD, we will choose 3 features.

The coefficients in the best subset with 3 features are `failures`, `schoolsup` and `absences` -- not dissimilar from results from previous studies. Depending on which exact split is used, other features may result in this answer, indicating that the results may not be very robust.
 
```{r best-subset, results='hide'}
library(leaps)

# Specify the predictor variables you want to consider
predictors <- names(df_both %>% select(-final_grade, -ends_with("_por"), -ends_with("_mat")))  # Exclude the target variable

predictions <- c()
# Write a function to repeat with each fold
bestsubset <- function(data, pred, i) {
    # Subset data into training and testing sets based on the folds
      train_data <- data[folds[[i]], ]
      test_data <- data[-folds[[i]], ]

    # Convert the target variable to a matrix or vector ***using the fold**
    target <- as.matrix(train_data$final_grade)
    
    # Run the best subset selection from leaps
    subset_model <- leaps::regsubsets(target ~ ., data = train_data[, pred])
    
    return(subset_model)
  
}
bestsub <- bestsubset(df_both, predictors, 2) #using custom function from above
coef(bestsub, 3)
```
```{r eval-bestsub, results="hide", fig.show='hide', eval=FALSE}
# Run a loop to evaluate how large the model will be using 3-fold cross validation

for(i in 1:3){
  bestsub <- bestsubset(df_both, predictors, i)
  m <- summary(bestsub)$bic
  plot(1:8, m) + 
    abline(h= mean(m)-sd(m))
}
```


```{r final-lm, results='hide'}
lm_bestsub <- lm(final_grade ~ absences_mean + failures + schoolsup, data = train_data)
yhat_bestsub <- predict(lm_bestsub, test_data)
y_obs <- test_data$final_grade

runXval <- function(i){
  train_data <- df_both[folds[[i]], ]
  test_data <- df_both[-folds[[i]], ]
  lm <- lm(final_grade ~ sex_F + failures + parentEdu, data = train_data)

  predict <- predict(lm, test_data)
  r.squared <- summary(lm)$r.squared
  cor <- cor(predict, test_data$final_grade)
  metrics <- c(cor, r.squared)
  return(metrics)
}

cors <- c()
r.sq <- c()

for (i in 1:3){
  cors_bestsub <- c(cors, runXval(i)[[1]])
  r.sq_bestsub <- c(r.sq, runXval(i)[[2]])
}

mean(cors_bestsub)
mean(r.sq_bestsub)
cor_bestsub <- cor(yhat_bestsub, y_obs)

#plot(y_obs, yhat_bestsub)
```
Using a 3-fold validation, the mean correlation between predicted and actual final grades is `r round(mean(cors_bestsub), 2)` and the R squared is `r round(mean(r.sq_bestsub, 2))*100`%.

```{r final_failures}
df_both %>% 
  ggplot(aes(y=final_grade, x=absences_mean, alpha=failures, color=factor(schoolsup))) + 
  geom_point() +
  xlab("Absences")+
  ylab("Final Grade") + 
  ggtitle("Final Grade by Previously Failed, School Support, and Absences")
```
```{r results='hide'}
stats::aov(final_grade ~ failures + absences_mean + schoolsup, df_both) %>%
summary()

```
We can also attempt to use stepwise regression to see if there is a different result. Interestingly, this results in a different and much larger model -- perhaps because it is judged on AIC rather than BIC, which penalizes the inclusion of additional features more.



```{r echo=FALSE, results='hide'}
# Create full model
lm_full <- lm(final_grade ~ ., data = train_data)

# Use stepwise to select features 
lm_step = stepAIC(lm_full, direction = "both", trace=0)
summary(lm_step)

# Predict results to evaluate and validate

yhat_step <- predict(lm_step, test_data)
cor_step <- cor(yhat_step, test_data$final_grade)
rmse_step <- rmse(test_data$final_grade, yhat_step)
rmse_bestsub <- rmse(test_data$final_grade, yhat_bestsub)
#plot(y_obs, yhat_step)
```

In fact the result is very similar with a  correlation between test and predicted results of `r cor_step` and multiple R-squared of `r round(summary(lm_step)$adj.r.squared, 2)`. RMSE is close but higher at `r rmse_step`compared to `r rmse_bestsub`

### Model 2

**Random Forest Model**

```{r echo=FALSE}

model_rf <- randomForest(final_grade ~ ., data = train_data)

model_rf

opar <- par(mfrow=c(1,2))

plot(model_rf)

varImpPlot(model_rf)

par(opar)

dev.off() 
```

```{r rf_eval, echo=FALSE, results="hide"}
yhat_rf <- predict(model_rf, newdata = test_data)

rmse_rf <- rmse(y_obs, yhat_rf)
cor_rf <- cor(y_obs, yhat_rf)
#plot(y_obs, yhat_rf)
```

### Model 3

**Decision tree model**

```{r echo=FALSE}
data_tr <- rpart(final_grade ~ ., data = train_data)

data_tr

opar <- par(mfrow = c(1,1), xpd = NA)

plot(data_tr)

text(data_tr, use.n = TRUE)

par(opar) 

```

```{r echo=FALSE, results='hide'}
yhat_tr <- predict(data_tr,  newdata = test_data)
cor_tr <- cor(y_obs, yhat_tr)
rmse_tr <- rmse(y_obs, yhat_tr)
plot(y_obs, yhat_tr)
```

In a complex model, a decision tree can be effective. This model results in correlation between predicted and actual final grades of `r cor_tr` as well as an RMSE of `r rmse_tr`.

## Model Selection

The best model is not one with the highest R-squared, but the one that performs the best on test data. In fact, all of these models performed very similarly.

```{r}
boxplot(list(lm_bestsub = y_obs - yhat_bestsub,
             lm_step = y_obs-yhat_step,
             rf = y_obs-yhat_rf,
             tr = y_obs - yhat_tr), 
        ylab="Error in Validation Set",
        title = "Error by Model")

abline(h=0, lty=2, col='blue')

results <- tibble(y_obs, yhat_bestsub, yhat_step, yhat_tr, yhat_rf)

Models <- c("Best Subset", "Stepwise Selection", "Random Forrest", "Decision Tree")
Correlation <- c(cor_bestsub, cor_step, cor_rf, cor_tr)
RMSE <- c(rmse_bestsub, rmse_step, rmse_rf, rmse_tr)

compare <- tibble(Models, Correlation, RMSE)

compare

results %>% gather(key, value, -y_obs) %>% ggplot(aes(x=value, y=y_obs)) + geom_point() + facet_grid(~key)
```


## Conclusion

The most significant predictors in the Best Subset model for the students final grade turns out to be having previously failed a class, presence or absence of school support, and absences. This is an interesting and actionable set of results, as students who have previously failed are a known population to whom extra school support can be given. School support, thankfully, also rises to the top of the predictors. The best predictive model by a hair in terms of correlation between predicted and actual results, is the Random Forest, but the differences are in fact extremely subtle and unlikely to be replicated exactly. Decision Tree and Random Forest modeling are harder to interpret or explain.

Ultimately, the choice of the best model depends upon the goal of the study. For a policy maker, the simplest linear model may be preferred as it results in the clearest and most defensible policy prescriptions:
* Intervene with students who have previously failed
* Offer school support
* Intervene to prevent or mitigate absences


## Appendix 1: Data Dictionary
Adapted from Kaggle: https://www.kaggle.com/datasets/uciml/student-alcohol-consumption

* *absences* - number of school absences (numeric: from 0 to 93)
* *absences_mean* - mean reported absences from both classes for students in Portuguese and Math
* *activities* - extra-curricular activities (binary: yes or no)
* *address* - student's home address type (binary: 'U' - urban or 'R' - rural)
* *alc* - combined `Dalc` + `Walc`
* *age* - student's age (numeric: from 15 to 22)
* *Dalc*  - workday alcohol consumption (numeric: from 1 - very low to 5 - very high) *removed
* *failures* - number of past class failures (numeric: n if 1<=n<3, else 4)
* *famsize* - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)
* *famsup* / *famEdsup*- family educational support (binary: yes or no) *renamed
* *Fedu* - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education) *removed
* *Fjob* - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
* *famrel* - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
* *freetime* - free time after school (numeric: from 1 - very low to 5 - very high) *renamed
* *goout* / *friendtime* - going out with friends (numeric: from 1 - very low to 5 - very high)
* *guardian* - student's guardian (nominal: 'mother', 'father' or 'other')
* *health* - current health status (numeric: from 1 - very bad to 5 - very good)
* *higher* - wants to take higher education (binary: yes or no)
* *internet* - Internet access at home (binary: yes or no)
* *Medu* - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ * *higher* education) *removed
* *Mjob* - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
* *nursery* - attended nursery school (binary: yes or no)
* *parentEdu* - sum of mother's + fathers education level
* *paid* - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
* *Pstatus* - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)
* *reason* - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')
* *romantic* - with a romantic relationship (binary: yes or no)
* *school* - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)
* *schoolsup* - extra educational support (binary: yes or no)
* *sex* - student's sex (binary: 'F' - female or 'M' - male)
* *studytime* - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)
* *traveltime* - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)
* *Walc*  - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high) *removed

These grades are related with the course subject, Math or Portuguese and are dropped for the joint analysis:

*G1* - first period grade (numeric: from 0 to 20)
*G2* - second period grade (numeric: from 0 to 20)
*G3* - final grade (numeric: from 0 to 20)

This is the target variable:
*final_grade* - final grade averaged from both classes (numeric: from 0 to 20, output target)

## Appendix 2: Summary Statistics
### `df_both`
```{r summary}
descrip
```

## Appendix 3: Basic Plots

#### Students in Both Classes
```{r basicPlots}

basicp <- function(df, color){
df_long <- df %>% select_if(is.numeric)
p <- df_long %>% gather() %>% ggplot(aes(x= value)) + geom_histogram(stat = "count", fill = color) + facet_wrap(~key, scales = 'free')
print(p)
}

basicp (df_both, "skyblue")
```

#### Students in Math
```basicp(df_mat, "pink")```

#### Students in Portuguese
```basicp(df_por, "darkblue")```

## Appendix 3: References

Data source accessed: Kaggle competition https://www.kaggle.com/datasets/uciml/student-alcohol-consumption 

Data source original citation: P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th Future Business Technology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7. Accessed via [Kaggle](https://www.kaggle.com/datasets/uciml/student-alcohol-consumption)

#### Title: Application of Multiple Linear Regression Identifying Contributing Factors in Studentsâ€™Academic Achievement
Authors: Dg Siti Nurisya Sahirah Binti Ag Isha and Siti Rahayu Binti Mohd Hashim
Proceedings of the International Conference on Mathematical Sciences and Statistics 2022 (ICMSS 2022)At: Selangor, Malaysia
Mathematics With Economics Programme, Faculty of Science and Natural Resources, December 2022
https://www.researchgate.net/publication/366929441_Application_of_Multiple_Linear_Regression_in_Identifying_Contributing_Factors_in_Students%27_Academic_Achievement

This study aimed to identify significant factors that contribute to students' academic success by analyzing internal and external factors. The study involved 327 final-year undergraduate students and found that self-esteem, intelligence, and maternal education were significant factors affecting students' achievement. 

In this research, they made three different models to see which factors were most important for academic achievement. They found that self-esteem, IQ, and maternal education were the most important factors. 


#### A Study on Academic Achievement and Personality of Secondary School Students
Authors: Dr. Suvarna V. D.and Dr H. S. Ganesha Bhata1
Research in Pedagogy, v6 n1 p99-108, 2016
https://files.eric.ed.gov/fulltext/EJ1149330.pdf

This study uses ANOVA and the Pearson's product-moment coefficient to test hypotheses related to differences in academic achievement between different groups of students (including both demographics and responses to a personality test ) using a data set of approximately 300 secondary school students.The results find that the students' age and gender affect their achievement levels, but that other demographic categories and (interestingly) personality characteristics do not.

### Predictors of Academic Performance in High School Students: The Longitudinal ASAP Study
Authors: Marie-Maude Dubuc, Mylene Aubertin-Leheudr, and Antony D. Karelis
Published online 2022 May 1, International Journal of Exercise
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9365103/#:~:text=Finally%2C%20psychological%20factors%20such%20as,42%2C%2043%2C%2048)

This study used moderated multivariate linear regression, separately comparing male and female students, to evaluate the impact of social, physical, and cognitive factors to explain variation in academic achievement among a cohort of 185 high-school students evaluated at a single high school over three years. The researchers controlled for demographic factors such as race, income, and ethnicity and use both a cross-sectional and longitudinal approach. In their results, they found that sex, cardiovascular fitness (measured by VO2 Max), and working memory were important predictors; however, they found that results differed when controlling for sex, school subject, and study design indicating that academic performance is in fact a highly complex phenomenon.

## Appendix 4: Data Visualization
### Romantic status

In general, students who are in a romantic relationship may have a more challenging time focusing on their studies compared to those who are not. A plot indicates that this may be the case -- especially male students who are not romantically involved appear to have a slightly higher final grade; ANOVA confirms that romantic status and sex both have significant predictive value.

```{r echo=FALSE}
# Calculate average of final_grade for each combination of romantic status and sex
df_both %>% 
  ggplot( aes(factor(sex_F), final_grade, fill = factor(romantic))) + 
  geom_boxplot() + 
  xlab("Sex") + 
  ylab("Final Grade") + 
  ggtitle("Final Grade by Sex & Romantic Status")
```
```{r results='hide'}
stats::aov(final_grade ~ romantic + sex_F, df_both) %>%
summary()

```

### Study Time

Although at first glance the findings suggest that students who study for a longer duration have a higher average score compared to those who study for a shorter duration, study time is not predictive according to ANOVA and we cannot rule out chance.

```{r echo=FALSE}
df_both %>% 
  ggplot(aes(y=final_grade, x=factor(studytime), fill=factor(studytime))) + 
  geom_boxplot() + 
  xlab("Study Time") + 
  ylab("Final Grade") + 
  ggtitle("Final Grade by Study Time")
```
```{r results='hide'}
stats::aov(final_grade ~ studytime, df_both) %>%
summary()
```

### Internet

The following graph indicates that the difference in academic performance between students with and without Internet access is negligible; again, however, ANOVA results indicate that we cannot rule out the null hypothesis that chance may explain the difference in means between the two groups.

```{r echo=FALSE}
df_both %>% 
  ggplot(aes(y=final_grade, x=factor(internet), fill=factor(internet))) + 
  geom_boxplot() + 
  xlab("Access to Internet") + 
  ylab("Final Grade") + 
  ggtitle("Final Grade by Internet Access")
```
```{r results='hide'}
stats::aov(final_grade ~ internet, df_both) %>%
summary()

```

### Alcohol

The following graph indicates that the difference in academic performance between students with and without Internet access is negligible; again, however, ANOVA results indicate that we cannot rule out the null hypothesis that chance may explain the difference in means between the two groups.

```{r echo=FALSE}
df_both %>% 
  ggplot(aes(y=final_grade, x=factor(alc), fill=factor(alc))) + 
  geom_boxplot() + 
  xlab("Level of Alcohol Consumption") + 
  ylab("Final Grade") + 
  ggtitle("Final Grade by Alcohol Consumption")
```
```{r results='hide'}
stats::aov(final_grade ~ alc, df_both) %>%
summary()

```

